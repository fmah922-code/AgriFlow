{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation 1: Removing any collections from MongoDB that have ambiguous names. \\\n",
    "Transformation 2: Dropping off the first row which was a dummy null row\\\n",
    "Transformation 3: Conversion collection columns to more appropriate datatypes. (not all strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, DateType, FloatType, DecimalType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.getcwd().replace('\\\\scripts','')\n",
    "sys.path.insert(0, config_path)\n",
    "\n",
    "from config import settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_conn = MongoClient(settings.mongo_client)[settings.mongo_default_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_id(row):\n",
    "    return str(row['_id'])\n",
    "\n",
    "def drop_first_row(collection):\n",
    "    altered_col = collection.iloc[1:]\n",
    "    return altered_col \n",
    "\n",
    "spark_rdd_list =[]\n",
    "def populate_spark_rdd_list():\n",
    "    for collection in mongo_conn.list_collection_names():\n",
    "        test_df = drop_first_row(pd.DataFrame([item for item in mongo_conn.get_collection(collection).find()]))\n",
    "\n",
    "        #Converts id column to a string to make it easier to be created as a pyspark rdd.\n",
    "        test_df['id'] = test_df.apply(stringify_id, axis=1)\n",
    "        test_df = test_df.drop('_id',axis=1)\n",
    "\n",
    "        #Moves id to the front of the dataframe\n",
    "        id = test_df['id']\n",
    "        test_df.drop(labels=['id'], axis=1,inplace=True)\n",
    "        test_df.insert(0, 'id', id)\n",
    "\n",
    "\n",
    "        spark_df = spark.createDataFrame(test_df)\n",
    "\n",
    "        spark_df = spark_df.where(~col('Value').like('%(%'))\n",
    "        spark_df = spark_df.where(~col('CV (%)').like('%(%'))\n",
    "\n",
    "        spark_df = spark_df.withColumn(\"Value\",\n",
    "                                spark_df['Value']\n",
    "                                .cast('float')) \\\n",
    "                            .withColumn(\"CV (%)\",\n",
    "                                spark_df['CV (%)']\n",
    "                                .cast('float')) \\\n",
    "                            .withColumn('year',\n",
    "                               spark_df['year']\n",
    "                               .cast('int')) \\\n",
    "                            .withColumn('zip_5',\n",
    "                               spark_df['zip_5'] \\\n",
    "                               .cast('int'))\n",
    "\n",
    "        spark_rdd_list.append(spark_df)\n",
    "\n",
    "populate_spark_rdd_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
