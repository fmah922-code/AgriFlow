{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pendulum\n",
    "import re\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from airflow.decorators import dag, task\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.getcwd().replace('\\\\airflow','')\n",
    "sys.path.insert(0, config_path)\n",
    "\n",
    "from config import settings\n",
    "from scripts import data_extraction as de\n",
    "from scripts import data_transform as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    try:\n",
    "        de.drop_db_if_exists(settings.mongo_default_db)\n",
    "\n",
    "        mongo_instance = de.MongoDB(username= settings.mongo_username,\n",
    "                                password= settings.mongo_password, \\\n",
    "                                default_db = settings.mongo_default_db, \\\n",
    "                                default_col = settings.mongo_default_colname, \\\n",
    "                                default_clusterName= settings.mongo_default_clusterName, \\\n",
    "                                schema = settings.mongo_default_schema)\n",
    "    \n",
    "        mongo_instance.test_connectivity()\n",
    "        mongo_instance.initialize()\n",
    "\n",
    "        data = de.USDA_API(settings.usda_key)\n",
    "        data.add_params('state_alpha','US')\n",
    "\n",
    "        for commodity_desc in data.get_param_values('commodity_desc'):\n",
    "            for year in de.create_mongo_year_list(2015):\n",
    "                try:\n",
    "                    col_title = re.sub(r\"[ ,&()]\",\"\", commodity_desc).replace(\" \", \"_\")\n",
    "                    data.add_params('commodity_desc', commodity_desc)\n",
    "                    data.add_params('year', year)\n",
    "\n",
    "                    current_doc = data.call()\n",
    "                    connection = mongo_instance.test_connectivity()\n",
    "\n",
    "                    if  mongo_instance.test_connectivity() == '1' and type(current_doc) != str:\n",
    "                        mongo_instance.add_new_col(col_title)\n",
    "                        mongo_instance.add_record(current_doc, col_title)\n",
    "                        mongo_instance.drop_col(settings.mongo_default_colname)\n",
    "\n",
    "                    data.remove_params('commodity_desc')\n",
    "                    data.remove_params('year')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {e}, {data.call()}\")\n",
    "                    data.remove_params('commodity_desc')\n",
    "                    data.remove_params('year')   \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(extract_success: bool):\n",
    "    if extract_success == True:\n",
    "        spark = SparkSession.builder \\\n",
    "                .config(\"spark.jars\", \"C:\\jdbc\\postgresql-42.7.5.jar\") \\\n",
    "                .getOrCreate()\n",
    "    \n",
    "        mongo_conn = MongoClient(settings.mongo_client)[settings.mongo_default_db]\n",
    "\n",
    "        spark_rdd_list={}\n",
    "        for collection in mongo_conn.list_collection_names():\n",
    "            if collection not in settings.excluded_commodities:\n",
    "                test_df = dm.drop_first_row(pd.DataFrame([item for item in mongo_conn.get_collection(collection).find()]))\n",
    "\n",
    "                #Converts id column to a string to make it easier to be created as a pyspark rdd.\n",
    "                test_df['id'] = test_df.apply(dm.stringify_id, axis=1)\n",
    "                test_df = test_df.drop('_id',axis=1)\n",
    "\n",
    "                #Moves id to the front of the dataframe\n",
    "                id = test_df['id']\n",
    "                test_df.drop(labels=['id'], axis=1,inplace=True)\n",
    "                test_df.insert(0, 'id', id)\n",
    "\n",
    "\n",
    "                spark_df = spark.createDataFrame(test_df)\n",
    "\n",
    "                spark_df = spark_df.where(~col('Value').like('%(%'))\n",
    "                spark_df = spark_df.where(~col('CV (%)').like('%(%'))\n",
    "\n",
    "                spark_df = spark_df.withColumn(\"Value\",\n",
    "                                spark_df['Value']\n",
    "                                .cast('float')) \\\n",
    "                            .withColumn(\"CV (%)\",\n",
    "                                spark_df['CV (%)']\n",
    "                                .cast('float')) \\\n",
    "                            .withColumn('year',\n",
    "                               spark_df['year']\n",
    "                               .cast('int')) \\\n",
    "                            .withColumn('zip_5',\n",
    "                               spark_df['zip_5'] \\\n",
    "                               .cast('int')) \\\n",
    "                            .withColumn('load_time',\n",
    "                               spark_df['load_time'] \\\n",
    "                               .cast('date')) \\\n",
    "                            \n",
    "\n",
    "            spark_rdd_list[collection] = spark_df\n",
    "        return spark_rdd_list\n",
    "    else:\n",
    "        return 'Upstream workflow failure!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(collection_dict):\n",
    "    try:\n",
    "        if type(collection_dict) == dict:\n",
    "            for collection in collection_dict:\n",
    "                collection_dict[collection].write.format(\"jdbc\")\\\n",
    "                .mode('overwrite') \\\n",
    "                .option(\"url\", \"jdbc:postgresql://localhost:5432/USDA_DB\") \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"dbtable\", f\"ag.{collection.replace('-','_')}\") \\\n",
    "                .option(\"user\", f\"{settings.pgadmin_user}\").option(\"password\", f\"{settings.pgadmin_password}\") \\\n",
    "                .save()\n",
    "\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_success = extract()\n",
    "collection_dict = transform(extract_success)\n",
    "load(collection_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
